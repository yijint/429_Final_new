{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2154c6cd-1a03-44cb-90d7-e8d5caa8ac42",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ensemble model\n",
    "The ensemble combines five models:\n",
    "1. best L1 value\n",
    "2. best L2 value\n",
    "3. augmented \n",
    "4. best dropout value \n",
    "5. best model architecture (TBD)\n",
    "\n",
    "and compares the ensemble performance to the individual models' performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf239be2-e01a-4b3a-869b-f4d554146b0b",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed55ba98-da3b-4d05-87f1-b185f5cb4071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from preprocess import holdout_set\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score\n",
    "import pytorch_i3d_1lesslayer\n",
    "# for running in notebooks only:\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae358e8b-ba65-4489-8c4d-e0665dabe796",
   "metadata": {},
   "source": [
    "Load best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "110ed83f-2108-45ba-9d7e-cb8103e44e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"/home/jt9744/COS429/429_Final_new/ensemble_models/\"\n",
    "L1_name = \"30epochs_l1_lr_0.1_ld_0.0001\"\n",
    "L2_name = \"30epochs_l2_lr_0.1_wd_1e-11\"\n",
    "augment_name = \"60epochs_l2_lr_0.1_wd_1e-07_augment\"\n",
    "dropout_name = \"30epochs_l1_lr_0.1_ld_1e-07_dropout_06\" \n",
    "architecture_name = \"30epochs_l2_lr_0.1_wd_1e-07_1lesslayer\" \n",
    "baseline_name = \"30epochs_l2_lr_0.1_wd_1e-07\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d65c74a-6177-4e25-9dc7-aea301600140",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_L1 = torch.load(models_path + L1_name)\n",
    "best_L2 = torch.load(models_path + L2_name)\n",
    "best_augment = torch.load(models_path + augment_name)\n",
    "best_dropout = torch.load(models_path + dropout_name)\n",
    "best_architecture = torch.load(models_path + architecture_name)\n",
    "baseline = torch.load(\"/scratch/network/hishimwe/models/\" + baseline_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f598f0-7d0a-456e-8f9c-94360f62c2ab",
   "metadata": {},
   "source": [
    "Build dataset class for loading data later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "715fef14-5bc6-4591-adbb-eb4480f5ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, paths, v_names, v_labels, num_samples=16, transforms=None): # num_samples cannot be lower than 16\n",
    "        self.num_samples = num_samples\n",
    "        self.frames = dict()\n",
    "        for p in paths:\n",
    "            self.frames[p] = sorted(glob.glob(p+\"/*.jpg\"))\n",
    "        self.data = paths\n",
    "        self.video_names = v_names\n",
    "        self.video_labels = v_labels\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # get original video\n",
    "        p = self.data[idx]\n",
    "        \n",
    "        # sample frames uniformly and create newly sampled video \n",
    "        num_frames = len(self.frames[p])-1\n",
    "        sampled_idx = np.linspace(0, num_frames, self.num_samples) #get num_samples frames from the video\n",
    "        images = []\n",
    "        index = np.where(self.video_names == p.split('/')[-1]) #index of p's video name in video_names\n",
    "        label_video = self.video_labels[index] # the labels for the video\n",
    "        for i in sampled_idx:\n",
    "            image = torchvision.io.read_image(self.frames[p][int(i)])\n",
    "            small_dim = min(image.shape[-2:])\n",
    "            image = torchvision.transforms.functional.center_crop(image, (small_dim, small_dim))\n",
    "            image = torchvision.transforms.functional.resize(image, (224, 224), antialias=True)\n",
    "            images.append(image)\n",
    "        images = torch.stack(images, axis=1)\n",
    "        \n",
    "        # data augmentation \n",
    "        if (self.transforms is not None):\n",
    "            images = np.array(self.transforms(images.numpy()))\n",
    "            # normalize\n",
    "            images = (images/255)*2 - 1 # values are between -1 and 1\n",
    "            return torch.from_numpy(images).type(torch.FloatTensor), label_video \n",
    "        \n",
    "        else: \n",
    "            images = (images/255)*2 - 1 #values are between -1 and 1\n",
    "            return images, label_video \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd1ea4d-3583-4ee5-92f3-ba11a0ddf680",
   "metadata": {},
   "source": [
    "Extract videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "454f5919-ec35-40bd-ab46-7d7f0be24beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_paths = [\"IMG_8595-Copy1.MOV\", \"IMG_8595-Copy2.MOV\", \"IMG_8595-Copy3.MOV\",\n",
    "#                \"IMG_8595-Copy4.MOV\", \"IMG_8595-Copy5.MOV\", \"IMG_8595-Copy6.MOV\",\n",
    "#                \"IMG_8595-Copy7.MOV\", \"IMG_8595-Copy8.MOV\", \"IMG_8595-Copy9.MOV\",\n",
    "#                \"IMG_8595.MOV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7de25d6a-f06d-4fa2-a3e2-b26a58c7ecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the validation video names and labels\n",
    "_, video_val, _, label_val, unique_labels = holdout_set(0.25) \n",
    "batch_size = 10 # batch size when loading data\n",
    "num_videos_val = len(video_val)\n",
    "num_classes = len(set(label_val)) # count unique labels (11 classes)\n",
    "\n",
    "# extract validation video paths \n",
    "video_frames_path = \"/scratch/network/hishimwe/image\" \n",
    "paths = glob.glob(video_frames_path+\"/*\")\n",
    "random.seed(0)\n",
    "random.shuffle(paths)\n",
    "good_paths_val = list(filter(lambda c: c.split('/')[-1] in video_val, paths)) \n",
    "\n",
    "# construct dataset and dataloader\n",
    "d_val = dataset(paths=good_paths_val, v_names=video_val, v_labels= label_val)\n",
    "loader_val = torch.utils.data.DataLoader(d_val, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3baacf-c4d4-42d2-8f7e-0739d3a921c6",
   "metadata": {},
   "source": [
    "Create ensemble model from all the best models by averaging their predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab8b3f67-378b-4d8e-955e-82c2350cd5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(model, loader, num_classes, save_name):\n",
    "    ground_truth = []\n",
    "    logits = []\n",
    "\n",
    "    for data, label in loader:\n",
    "        torch.cuda.empty_cache()\n",
    "        data = data.cuda()\n",
    "        label = label.squeeze().type(torch.LongTensor).cuda()\n",
    "        per_frame_logits = model(data).mean(2)\n",
    "        \n",
    "        ground_truth.extend(list(label.cpu().detach().numpy()))\n",
    "        logits.extend(per_frame_logits.tolist())\n",
    "    \n",
    "    np.savetxt(models_path+'LOGITS_'+save_name, np.array(logits))\n",
    "    np.savetxt(models_path+'TRUTH_'+save_name, np.array(ground_truth))\n",
    "    \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2f9f456-4f90-4974-b76c-a2f159fad5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.8 s, sys: 8.58 s, total: 29.4 s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%time get_logits(model=baseline, loader=loader_val, num_classes=num_classes, save_name=baseline_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08959331-ac11-4f64-b2ee-61d9e6608c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.5 s, sys: 9.48 s, total: 30 s\n",
      "Wall time: 2min 23s\n"
     ]
    }
   ],
   "source": [
    "%time get_logits(model=best_L1, loader=loader_val, num_classes=num_classes, save_name=L1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "070fcd98-3dd7-4bcd-b4ec-30be9db6891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.8 s, sys: 8.59 s, total: 28.4 s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%time get_logits(model=best_L2, loader=loader_val, num_classes=num_classes, save_name=L2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89748dba-667d-4b21-a073-bf5eea8b7050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.8 s, sys: 8.51 s, total: 28.3 s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%time get_logits(model=best_augment, loader=loader_val, num_classes=num_classes, save_name=augment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee12eec0-fb74-4c7d-b531-b5bb67e7d7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.9 s, sys: 8.45 s, total: 28.4 s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%time get_logits(model=best_dropout, loader=loader_val, num_classes=num_classes, save_name=dropout_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48aac786-1aa9-4460-8c5c-94e2788ca3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.5 s, sys: 7.85 s, total: 27.3 s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%time get_logits(model=best_architecture, loader=loader_val, num_classes=num_classes, save_name=architecture_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cos429",
   "language": "python",
   "name": "cos429"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
